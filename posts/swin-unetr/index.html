<!doctype html><html lang=en-us><head><title>Swin UNETR Note | h(x)</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Swin UNETR is a model using Swin Transformer and a U-shaped network architecture to perform medical image segementation. Its official implementation is available here. In this memo I write down some key points. Input and Output üîóThe input is a $(N, C_i, D, H, W)$ tensor, in which $N$ is the batch size. $C_i$ is the number of input features. $D, H, W$ are the size of"><meta name=generator content="Hugo 0.102.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><nav class=navigation><a href=/><span class=arrow>‚Üê</span>Home</a>
<a href=/posts>Archive</a>
<a href=/tags>Tags</a>
<a href=/about>About</a></nav><main class=main><section id=single><h1 class=title>Swin UNETR Note</h1><div class=tip><time datetime="2022-09-08 00:00:00 +0000 UTC">Sep 8, 2022</time>
<span class=split>¬∑</span>
<span>1856 words</span>
<span class=split>¬∑</span>
<span>4 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#swin-transformer>Swin Transformer</a><ul><li><a href=#patch-embedding>Patch Embedding</a></li><li><a href=#shifted-window-based-self-attention>Shifted Window based Self-Attention</a></li><li><a href=#patch-merging>Patch Merging</a></li></ul></li><li><a href=#encoder>Encoder</a></li><li><a href=#decoder>Decoder</a></li></ul></nav></div></details></aside><div class=content><p><a href=https://arxiv.org/abs/2201.01266 target=_blank rel=noopener>Swin UNETR</a> is a model using <a href=https://arxiv.org/abs/2103.14030 target=_blank rel=noopener>Swin Transformer</a> and a U-shaped network architecture to perform medical image segementation. Its official implementation is available <a href=https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BRATS21 target=_blank rel=noopener>here</a>. In this memo I write down some key points.</p><p><p class=markdown-image><img src="https://github.com/Project-MONAI/research-contributions/blob/main/SwinUNETR/BRATS21/assets/swin_unetr.png?raw=true" alt></p></p><h1 id=input-and-output>Input and Output <a href=#input-and-output class=anchor>üîó</a></h1><p>The input is a $(N, C_i, D, H, W)$ tensor, in which</p><ul><li>$N$ is the batch size.</li><li>$C_i$ is the number of input features.</li><li>$D, H, W$ are the size of the feature, i.e. the medical image.</li></ul><p>The output is a $(N, C_o, D, H, W)$ tensor, in which</p><ul><li>$N$ is the batch size.</li><li>$C_o$ is the number of output features, corresponding to the number of segmentation labels.</li><li>$D, H, W$ are the same as input.</li></ul><p>(From now on we will ignore the batch dimension.)</p><p>Take the <a href=https://www.synapse.org/#!Synapse:syn27046444/wiki/616992 target=_blank rel=noopener>BraTs21</a> challange as an exmaple. Each sample consists of four 3D images coming from different <a href=https://en.wikipedia.org/wiki/MRI_sequence target=_blank rel=noopener>mpMRI sequences</a> of the same region of interest(RoI):</p><ul><li>T1</li><li>T1ce</li><li>T2</li><li>FLAIR</li></ul><p><p class=markdown-image><img src=https://www.researchgate.net/publication/340699988/figure/fig3/AS:881307630444555@1587131521187/Axial-view-of-T1-T1ce-T2-and-Flair.png alt></p></p><p>The annotation is saved in a single 3D image consisting of three non-zero voxel values:</p><ul><li>The necrotic tumor core(NCR ÂùèÊ≠ªËÇøÁò§ÁªÜËÉû) = 1;</li><li>The peritumoral edema(ED Áò§Âë®Ê∞¥ËÇø) = 2;</li><li>The enhencing tumor(ET Â¢ûÂº∫ËÇøÁò§) = 4;</li></ul><p>while the expected prediction labels are combinations of the above:</p><ul><li>Tumor core(TC) = NCR + ET;</li><li>Whole tumor(WT) = NCR + ET + ED;</li><li>Enhancing tumor(ET) = ET.</li></ul><p><p class=markdown-image><img src="https://github.com/Project-MONAI/research-contributions/blob/main/SwinUNETR/BRATS21/assets/fig_brats21.png?raw=true" alt></p></p><p>Therefore, $C_i=4$ and $C_o=3$. In particular, the outputs are probabilities
$$
\mathbb{p}(c,i,j,k)\in[0,1]
$$</p><p>telling for the voxel at $(i, j, k)$ the probabiliy for it to be of label TC($c=0$), WT($c=1$) and ET($c=2$).</p><h1 id=model>Model <a href=#model class=anchor>üîó</a></h1><p>Let $D$ be the dimensionality of the problem. For clarity and generality, we set</p><ul><li>$\mathbf{D}=(d_1,d_2,\cdots,d_D)$ be the dimension of the input;<ul><li>$\mathbf{D}=(H,W)$ for $D=2$;</li><li>$\mathbf{D}=(C,H,W)$ for $D=3$.</li></ul></li><li>$\mathbf{p}=(p_1,p_2,\cdots,p_D)$ be the patch size along each dimension.</li></ul><p>The overall computation flow is as follows:<p class=markdown-image><img src=/images/swin-unetr/overall.svg alt></p></p><ul><li>Swin-transform the input $x$ to get $N(=5)$ hidden states with dimensions $$h_n:=\left(2^nC_h, \frac{\mathbf{D}_p}{2^n}\right)$$ where $n=0,1,\cdots,N-1$ and $C_h$ is an internal feature size.</li><li>Encode $h_n$ to $e_n$ of the same dimension.</li><li>Decode $e_{N-1}$ to $d_{N-1}$ of the same dimension.</li><li>Combine $e_n$ with $d_{n+1}$ to decode to $d_n$ of the same dimension, for $n = N-2, N-1,\cdots 0$.</li><li>Encode $x$ to $e$ of dimension $(C_h,\mathbf{D})$, then combine with $d_0$ to decode to $d$ of the same dimension.</li><li>Project $d$ to the output $y$ of dimension $(C_o,\mathbf{D})$.</li></ul><p>In the Swim Transforme step, the minimal units are (featurised) patches, rather than original voxels. Thoese patches are like tokens in natural langauge. Only in the very last stage are they joined with the original image at voxel granularity.</p><h2 id=swin-transformer>Swin Transformer <a href=#swin-transformer class=anchor>üîó</a></h2><p>When doing Swin Transform, we first turn the input into <em>patches</em> of size $\mathbf{p}$, then further paratition these patches into <em>windows</em> of size $\mathbf{w}$, in each of which self-attention is computed. Since calculating attention preserves dimensions, the output is further downsampled to half its spatial dimensions while <em>double</em>, not $2^D$ times, the feature dimension.</p><p><p class=markdown-image><img src=/images/swin-unetr/swin.svg alt></p></p><h3 id=patch-embedding>Patch Embedding <a href=#patch-embedding class=anchor>üîó</a></h3><p>Conceptually there are two steps:</p><ul><li>Partition, i.e. reshape, the input into patches of size $\mathbf{p}$ with feature dimension $C_i$.</li><li>Perform $C_h$ linear transformations on each patch to map them to hidden states.</li></ul><p>Practically, it is done by a <a href=https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py#L461 target=_blank rel=noopener>single convolution</a> with <code>in_channel</code> be $C_i$, <code>out_channel</code> be $C_h$, <code>kernal_size</code> be $\mathbf{p}$ and <code>stride</code> be $\mathbf{p}$.</p><h3 id=shifted-window-based-self-attention>Shifted Window based Self-Attention <a href=#shifted-window-based-self-attention class=anchor>üîó</a></h3><p>Let $\mathbf{w}$ be the window size and $\mathbf{\Delta}$ be the shift distance along each dimension. We require $\Delta_d&lt;w_d$ to ensure overlapping across windows, thus imply communication between them. Typically, we set $\Delta_d=w_d/2$.</p><p>Two ingredients are important:</p><ul><li>Since during shifting the window is padded cyclically by rolling patches at tail to head, an <em>attention mask</em> is required to tell which patches are in proximity in the original image, thus among which attention can be calculated. A patch should not pay attention to a padded patch moved from far away.</li><li>A <em>relative position bias</em> is introduced to put information of the relative position of two patches into attention.</li></ul><h4 id=attention-mask>Attention Mask <a href=#attention-mask class=anchor>üîó</a></h4><p><p class=markdown-image><img src=/images/swin-unetr/attention_mask.svg alt></p></p><p>As shown in the figure, the whole image is shifted by $\mathbf{\Delta}$ and padded cyclicly by patches 2, 5, 6, 7, 8.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>roll(x, shifts<span style=color:#f92672>=</span>shifts, dims<span style=color:#f92672>=</span>dims)
</span></span></code></pre></div><p>Now, in the newly shifted image, patches with same colors are in proximity in the original unshifted image, and should pay attention to only patches of the same color. (Note that although, say, 0 and 3 are close in the origional image, they belongs to differnet windows in the shifted image, thus are assigned different colors.)</p><p>To represent this, we calcualte an <strong>attention mask</strong> $m$ which is a $\left(N_w, V_w,V_w\right)$ dimensional tensor, in which $$N_w=\prod\frac{\mathbf{D}_p}{\mathbf{w}}$$ is the total number of windows, and $$V_w=\prod\mathbf{w}$$ is the volumn of each window, i.e. number of patches within each window.</p><p><p class=markdown-image><img src=/images/swin-unetr/windows.svg alt></p></p><p>Conceptually, we need to calcualte self-attention within each row of the above figure, but also limit the calculation to each color block.</p><p>The value of $m$ is
$$
m(n,i,j) = \begin{cases}
0 &\text{if patch $i$ and patch $j$ are relevant in window $n$} \\
-\infty &\text{otherwise}
\end{cases}
$$</p><p>This mask is added to the oridnary attention before calcualting softmax to effectively turn attentions among irrelevant patches to zero.</p><p><a href=https://github.com/Project-MONAI/MONAI/blob/342f4aa/monai/networks/nets/swin_unetr.py#L758-L795 target=_blank rel=noopener>In code</a> (also see <a href=https://github.com/microsoft/Swin-Transformer/blob/e43ac64/models/swin_transformer.py#L223-L241 target=_blank rel=noopener>here</a>), we first calculate a label tensor $l(n,i)$ of dimension $\left(N_w, V_w\right)$ with value in $\mathbb{Z}$ telling the label of patch $i$ in window $n$. Then we set $m(n,i,j)=l(n,i)-l(n,j)$, which can be efficiently calculated through <a href=https://pytorch.org/docs/stable/notes/broadcasting.html target=_blank rel=noopener>broadcasting</a>. Finally, we turn all nonzero entry of $m$ to a sufficiently negative large number.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>wd, wh, ww <span style=color:#f92672>=</span> window_size
</span></span><span style=display:flex><span>sd, sh, sw <span style=color:#f92672>=</span> shift_size
</span></span><span style=display:flex><span>inf <span style=color:#f92672>=</span> <span style=color:#ae81ff>100.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>d_ranges <span style=color:#f92672>=</span> (slice(<span style=color:#f92672>-</span>wd), slice(<span style=color:#f92672>-</span>wd, <span style=color:#f92672>-</span>sd), slice(<span style=color:#f92672>-</span>sd, <span style=color:#66d9ef>None</span>))
</span></span><span style=display:flex><span>h_ranges <span style=color:#f92672>=</span> (slice(<span style=color:#f92672>-</span>wh), slice(<span style=color:#f92672>-</span>wh, <span style=color:#f92672>-</span>sh), slice(<span style=color:#f92672>-</span>sh, <span style=color:#66d9ef>None</span>))
</span></span><span style=display:flex><span>w_ranges <span style=color:#f92672>=</span> (slice(<span style=color:#f92672>-</span>ww), slice(<span style=color:#f92672>-</span>ww, <span style=color:#f92672>-</span>sw), slice(<span style=color:#f92672>-</span>sw, <span style=color:#66d9ef>None</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># label patches by relevance</span>
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>d, h, w <span style=color:#f92672>=</span> dims
</span></span><span style=display:flex><span>img_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((d, h, w))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> d, h, w <span style=color:#f92672>in</span> itertools<span style=color:#f92672>.</span>product(d_ranges, h_ranges, w_ranges):
</span></span><span style=display:flex><span>    img_mask[d, h, w] <span style=color:#f92672>=</span> l
</span></span><span style=display:flex><span>    l <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># turn img_mask into a (N_w, V_w) tensor.</span>
</span></span><span style=display:flex><span>windows <span style=color:#f92672>=</span> img_mask<span style=color:#f92672>.</span>view(d <span style=color:#f92672>//</span> wd, wd, h <span style=color:#f92672>//</span> wh, wh, w <span style=color:#f92672>//</span> ww, ww)
</span></span><span style=display:flex><span>windows <span style=color:#f92672>=</span> windows<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, wd <span style=color:#f92672>*</span> wh <span style=color:#f92672>*</span> ww)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># calcualte attention mask</span>
</span></span><span style=display:flex><span>attn_mask <span style=color:#f92672>=</span> windows<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>-</span> windows<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>attn_mask <span style=color:#f92672>=</span> attn_mask<span style=color:#f92672>.</span>masked_fill(attn_mask <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#f92672>-</span>inf))<span style=color:#f92672>.</span>masked_fill(attn_mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#ae81ff>0.0</span>))
</span></span></code></pre></div><h4 id=relative-position-bias>Relative Position Bias <a href=#relative-position-bias class=anchor>üîó</a></h4><p>Along dimension $i$ there are $2w_i-1$ relative positions, ranging from $-(w_i-1)$ to $w_i-1$, thus in total there are
$$
R := \prod_{i=1}^{D}(2w_i-1)
$$
different possible relative positions in a window.</p><p>Given two positions in a window, $\mathbf{p}$ and $\mathbf{q}$, we need to encode their relative position to a number in $\mathbb{Z}_R$. When all $w_i$s are equal to some $w$, a straight-forward way is to view the relative position in a $(2w-1)$-base number system. In general we can do in a similar fashion:</p><p>$$
r(\mathbf{p},\mathbf{q}):=\sum_{i=0}^{D-1}\left(p_i-q_i+(w_i-1)\right)\prod_{j=i+1}^{D-1}\cdot(2w_j-1)
$$</p><p>where we add $w_i-1$ to $p_i-q_i$ to turn its value range from $[-(w_i-1),w_i)$ to $[0, 2w_i-1)$.</p><p>A <strong>relative position bias</strong> $B^{(k)}$ of dimension $\left(h^{(k)},R\right)$ is learned for $k$-th attention calculation, where $h^{(k)}$ is the number of heads of the attention.</p><p>The code to construct such tensor can be found <a href=https://github.com/microsoft/Swin-Transformer/blob/e43ac64/models/swin_transformer.py#L100-L115 target=_blank rel=noopener>here</a> and <a href=https://github.com/Project-MONAI/MONAI/blob/342f4aa/monai/networks/nets/swin_unetr.py#L439-L479 target=_blank rel=noopener>here</a>. Roughtly as</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>wd, wh, ww <span style=color:#f92672>=</span> window_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># relative_position_bias_table is a (R, h) tensor storing learnable parameters representing each relative position.</span>
</span></span><span style=display:flex><span>relative_position_bias_table <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>zeros(
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> wd <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> wh <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> ww <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>        num_heads
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>coords_d <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(wd)
</span></span><span style=display:flex><span>coords_h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(wh)
</span></span><span style=display:flex><span>coords_w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(ww)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># coords is a (3, wd, wh, ww) tensor where coords[n][i][j][k] = i, j, k when n = 0, 1, 2.</span>
</span></span><span style=display:flex><span>coords <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack(torch<span style=color:#f92672>.</span>meshgrid(coords_d, coords_h, coords_w, indexing<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ij&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># coords_flatten is a (3, V_w) tensor where coords_flatten[n] tells the n-th coordinate of every position in natural encoding from (i, j, k) to the unique index in [0, V_w), n = 0, 1, 2.</span>
</span></span><span style=display:flex><span>coords_flatten <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>flatten(coords, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># relative_coords is a (3, V_w, V_w) tensor where relative_coords[n][i][j] tells the *difference* of the n-th coordinate between position i and j.</span>
</span></span><span style=display:flex><span>relative_coords <span style=color:#f92672>=</span> coords_flatten[:, :, <span style=color:#66d9ef>None</span>] <span style=color:#f92672>-</span> coords_flatten[:, <span style=color:#66d9ef>None</span>, :]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># reshape to (V_w, V_w, 3)</span>
</span></span><span style=display:flex><span>relative_coords <span style=color:#f92672>=</span> relative_coords<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>contiguous()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># calculate the embedding into [0, R).</span>
</span></span><span style=display:flex><span>relative_coords[:, :, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+=</span> wd <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>relative_coords[:, :, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+=</span> wh <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>relative_coords[:, :, <span style=color:#ae81ff>2</span>] <span style=color:#f92672>+=</span> ww <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>relative_coords[:, :, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>*=</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> wh <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> ww <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>relative_coords[:, :, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>*=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> ww <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>relative_position_index <span style=color:#f92672>=</span> relative_coords<span style=color:#f92672>.</span>sum(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># relative_position_index is a (V_w, V_w) tensor where x := relative_position_index[i][j] is the embedded relation position in range [0, R).</span>
</span></span><span style=display:flex><span><span style=color:#75715e># relative_position_bias_table[x] then further gives a vector of length h as the bias for each of h headers.</span>
</span></span></code></pre></div><h4 id=attention>Attention <a href=#attention class=anchor>üîó</a></h4><p>The attention distribution is calculated as
$$
P^{(k)}(n,h,\mathbf{p},\mathbf{q})=\text{softmax}\left(\frac{Q^{(k)}\cdot \left(K^{(k)}\right)^\intercal}{\sqrt{h^{(k)}}}(n, h,\mathbf{p},\mathbf{q}) + B^{(k)}(\cdot,h,r(\mathbf{p},\mathbf{q}))+m(n,\cdot,\mathbf{p},\mathbf{q})\right)
$$
where indexing by $\mathbf{p}$ and $\mathbf{q}$ is understood under their natural mapping to $[0, V_w)$.</p><p>Finally, the attention is given by
$$
a^{(k)}=P^{(k)}\cdot V^{(k)}
$$</p><h3 id=patch-merging>Patch Merging <a href=#patch-merging class=anchor>üîó</a></h3><p>After finishing calculating window attention, which gives a $(N_w, V_w, C_h)$ tensor, we reshape it back to the original image, i.e. a $(C_h,\mathbf{D}_p)$ tensor $z$. Now we want to down sample it to a $(2C_h, \mathbf{D}_p/2)$ tensor.</p><p><p class=markdown-image><img src=/images/swin-unetr/patch_merging.svg alt></p></p><p>As demostrated in the figure, we split $z$ to $2^D$ smaller tensors of dimension $(C_h, \mathbf{D}_p/2)$ by sampling at step $2$, and then stack them to a $(2^DC_h, \mathbf{D}_p/2)$ tensor. Next, we perform a linear transformation to turn $2^DC_h$ to $2C_h$.</p><p>Code can be found <a href=https://github.com/Project-MONAI/MONAI/blob/342f4aa/monai/networks/nets/swin_unetr.py#L704-L725 target=_blank rel=noopener>here</a> and <a href=https://github.com/microsoft/Swin-Transformer/blob/e43ac64/models/swin_transformer.py#L331-L352 target=_blank rel=noopener>here</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x0 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x1 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x2 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x3 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x4 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x5 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x6 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x7 <span style=color:#f92672>=</span> x[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>, :]
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([x0, x1, x2, x3, x4, x5, x6, x7], <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>8</span> <span style=color:#f92672>*</span> dim, <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> dim, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)(reduction(x))
</span></span></code></pre></div><h2 id=encoder>Encoder <a href=#encoder class=anchor>üîó</a></h2><p>The encoder is a typical multi-layer residual convolutional neural network, with carefully setting <code>kernal_size</code> to <code>3</code>, <code>stride</code> to <code>1</code> and <code>padding</code> to <code>1</code>, thus preserving the dimensions.</p><p>Note that in general the output dimension of a convolution operator is given by:
$$
o=\left\lfloor\frac{i+2p-k}{s}\right\rfloor+1
$$
where</p><ul><li>$i$: input dimension;</li><li>$p$: padding on both side;</li><li>$k$: kernal size;</li><li>$s$: stide.</li></ul><p>Thus indeed when $s=p=1$ and $k=3$ we have $o=i$.</p><h2 id=decoder>Decoder <a href=#decoder class=anchor>üîó</a></h2><p>When decoding, we first use a <a href=https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11 target=_blank rel=noopener>transposed convolutional layer</a> to up sample the previously decoded tensor $d$ to double its dimensionality from $(C,\mathbf{D})$ to $(2C,2\mathbf{D})$. In general, the dimension of the output of a transposed convolutional is given by:
$$
o = (i-1)\cdot s + k-2p
$$
Thus by setting $k=s=2$ and $p=0$, we have $o=2i$.</p><p>After that, we concatenate it with the encoded tensor at the same level to get a $(4C,2\mathbf{D})$ tensor, and put it through another convolutional neural network to reduce $4C$ to $2C$ again.</p><h1 id=remark>Remark <a href=#remark class=anchor>üîó</a></h1><ul><li>As in the overall diagram, in order to be able to combine $e$ and $d_0$ to decode to $d$, it requires $\mathbf{D}$ is twice as much as $\mathbf{D}_p$, which further requires $\mathbf{p}$ to be $\mathbf{2}$. A relaxation maybe desired.</li></ul></div><div class=tags><a href=https://hxhxhx88.github.io/tags/medical-imaging>Medical Imaging</a>
<a href=https://hxhxhx88.github.io/tags/paper-note>Paper Note</a></div></section></main><footer id=footer><div class=copyright>¬© Copyright
2022
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg></span>h(x)</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-mini>nodejh</a></div></footer></body></html>